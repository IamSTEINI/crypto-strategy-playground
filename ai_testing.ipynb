{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "8f47a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "e7f10999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], requires_grad=True)\n",
      "torch.Size([2, 3])\n",
      "torch.float32\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "data = [[1.0,2.0,3.0], [4.0,5.0,6.0]]\n",
    "tensor = torch.tensor(data, requires_grad=True)\n",
    "\n",
    "print(tensor)\n",
    "\n",
    "print(tensor.shape)\n",
    "print(tensor.dtype)\n",
    "print(tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "7fb3f30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[-1.6835,  2.2160,  0.6732],\n",
      "        [-0.3262, -1.2150, -1.2803]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3)\n",
    "ones = torch.ones(shape)\n",
    "zeros = torch.zeros(shape)\n",
    "random = torch.randn(shape)\n",
    "\n",
    "print(ones)\n",
    "print(zeros)\n",
    "print(random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "3c24943f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(60., grad_fn=<MulBackward0>)\n",
      "<MulBackward0 object at 0x000001C15425AF20>\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(4.0, requires_grad=True)\n",
    "x = torch.tensor(10., requires_grad=True)\n",
    "\n",
    "y = a + b\n",
    "z = x * y\n",
    "\n",
    "print(z)\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "872bfdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.,  8.],\n",
      "        [ 3., 32.]])\n",
      "tensor([2., 3.])\n",
      "tensor([1.5000, 3.5000])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1.,2.],[3.,4.]])\n",
    "b = torch.tensor([[5.,4.],[1.,8.]])\n",
    "\n",
    "c = a * b\n",
    "print(c)\n",
    "print(a.mean(dim=0))\n",
    "print(a.mean(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "1ac80fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([ 2,  6, 10])\n",
      "tensor([3, 3, 3])\n",
      "tensor([[ 0],\n",
      "        [ 6],\n",
      "        [11]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12).reshape(3,4)\n",
    "col = x[:, 2]\n",
    "print(x)\n",
    "print(col)\n",
    "print(torch.argmax(x, dim=1))\n",
    "print(torch.gather(x, dim=1, index=torch.tensor([[0], [2], [3]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "3f0d6c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init. W: tensor([[1.0868]], requires_grad=True)\n",
      "Init. b: tensor([0.0944], requires_grad=True)\n",
      "Prediction y hat: tensor([[ 1.2978],\n",
      "        [-0.6659],\n",
      "        [ 1.3976]], grad_fn=<SliceBackward0>)\n",
      "True y: tensor([[ 3.2623],\n",
      "        [-0.1563],\n",
      "        [ 3.4464]])\n",
      "LOSS: 2.2548105716705322\n",
      "tensor([[-2.1489]])\n",
      "tensor([-2.6922])\n",
      "EPOCH: 00 \\ Loss: 10.8781, W: -1.547, b: 0.655\n",
      "EPOCH: 10 \\ Loss: 7.0956, W: -1.050, b: 0.994\n",
      "EPOCH: 20 \\ Loss: 4.7807, W: -0.645, b: 1.235\n",
      "EPOCH: 30 \\ Loss: 3.3376, W: -0.311, b: 1.401\n",
      "EPOCH: 40 \\ Loss: 2.4167, W: -0.035, b: 1.512\n",
      "EPOCH: 50 \\ Loss: 1.8119, W: 0.197, b: 1.581\n",
      "EPOCH: 60 \\ Loss: 1.4013, W: 0.392, b: 1.621\n",
      "EPOCH: 70 \\ Loss: 1.1126, W: 0.559, b: 1.638\n",
      "EPOCH: 80 \\ Loss: 0.9020, W: 0.702, b: 1.639\n",
      "EPOCH: 90 \\ Loss: 0.7432, W: 0.826, b: 1.630\n",
      "EPOCH: 100 \\ Loss: 0.6198, W: 0.935, b: 1.612\n",
      "EPOCH: 110 \\ Loss: 0.5216, W: 1.031, b: 1.589\n",
      "EPOCH: 120 \\ Loss: 0.4420, W: 1.115, b: 1.563\n",
      "EPOCH: 130 \\ Loss: 0.3763, W: 1.191, b: 1.536\n",
      "EPOCH: 140 \\ Loss: 0.3217, W: 1.258, b: 1.507\n",
      "EPOCH: 150 \\ Loss: 0.2759, W: 1.319, b: 1.479\n",
      "EPOCH: 160 \\ Loss: 0.2372, W: 1.374, b: 1.450\n",
      "EPOCH: 170 \\ Loss: 0.2045, W: 1.424, b: 1.423\n",
      "EPOCH: 180 \\ Loss: 0.1767, W: 1.469, b: 1.397\n",
      "EPOCH: 190 \\ Loss: 0.1530, W: 1.511, b: 1.372\n",
      "EPOCH: 200 \\ Loss: 0.1329, W: 1.549, b: 1.348\n",
      "EPOCH: 210 \\ Loss: 0.1157, W: 1.583, b: 1.326\n",
      "EPOCH: 220 \\ Loss: 0.1011, W: 1.615, b: 1.305\n",
      "EPOCH: 230 \\ Loss: 0.0886, W: 1.644, b: 1.286\n",
      "EPOCH: 240 \\ Loss: 0.0779, W: 1.671, b: 1.267\n",
      "EPOCH: 250 \\ Loss: 0.0688, W: 1.696, b: 1.250\n",
      "EPOCH: 260 \\ Loss: 0.0610, W: 1.718, b: 1.234\n",
      "EPOCH: 270 \\ Loss: 0.0544, W: 1.739, b: 1.219\n",
      "EPOCH: 280 \\ Loss: 0.0487, W: 1.759, b: 1.206\n",
      "EPOCH: 290 \\ Loss: 0.0438, W: 1.776, b: 1.193\n",
      "EPOCH: 300 \\ Loss: 0.0397, W: 1.793, b: 1.181\n",
      "EPOCH: 310 \\ Loss: 0.0362, W: 1.808, b: 1.170\n",
      "EPOCH: 320 \\ Loss: 0.0332, W: 1.822, b: 1.160\n",
      "EPOCH: 330 \\ Loss: 0.0306, W: 1.835, b: 1.151\n",
      "EPOCH: 340 \\ Loss: 0.0284, W: 1.847, b: 1.142\n",
      "EPOCH: 350 \\ Loss: 0.0265, W: 1.858, b: 1.134\n",
      "EPOCH: 360 \\ Loss: 0.0249, W: 1.868, b: 1.127\n",
      "EPOCH: 370 \\ Loss: 0.0235, W: 1.877, b: 1.120\n",
      "EPOCH: 380 \\ Loss: 0.0223, W: 1.886, b: 1.113\n",
      "EPOCH: 390 \\ Loss: 0.0213, W: 1.894, b: 1.108\n",
      "EPOCH: 400 \\ Loss: 0.0205, W: 1.901, b: 1.102\n",
      "EPOCH: 410 \\ Loss: 0.0198, W: 1.908, b: 1.097\n",
      "EPOCH: 420 \\ Loss: 0.0191, W: 1.915, b: 1.092\n",
      "EPOCH: 430 \\ Loss: 0.0186, W: 1.920, b: 1.088\n",
      "EPOCH: 440 \\ Loss: 0.0181, W: 1.926, b: 1.084\n",
      "EPOCH: 450 \\ Loss: 0.0178, W: 1.931, b: 1.080\n",
      "EPOCH: 460 \\ Loss: 0.0174, W: 1.935, b: 1.077\n",
      "EPOCH: 470 \\ Loss: 0.0171, W: 1.940, b: 1.074\n",
      "EPOCH: 480 \\ Loss: 0.0169, W: 1.944, b: 1.071\n",
      "EPOCH: 490 \\ Loss: 0.0167, W: 1.947, b: 1.068\n",
      "EPOCH: 500 \\ Loss: 0.0165, W: 1.951, b: 1.066\n",
      "EPOCH: 510 \\ Loss: 0.0164, W: 1.954, b: 1.064\n",
      "EPOCH: 520 \\ Loss: 0.0162, W: 1.957, b: 1.062\n",
      "EPOCH: 530 \\ Loss: 0.0161, W: 1.959, b: 1.060\n",
      "EPOCH: 540 \\ Loss: 0.0160, W: 1.962, b: 1.058\n",
      "EPOCH: 550 \\ Loss: 0.0160, W: 1.964, b: 1.056\n",
      "EPOCH: 560 \\ Loss: 0.0159, W: 1.966, b: 1.055\n",
      "EPOCH: 570 \\ Loss: 0.0158, W: 1.968, b: 1.053\n",
      "EPOCH: 580 \\ Loss: 0.0158, W: 1.970, b: 1.052\n",
      "EPOCH: 590 \\ Loss: 0.0157, W: 1.971, b: 1.051\n",
      "EPOCH: 600 \\ Loss: 0.0157, W: 1.973, b: 1.049\n",
      "EPOCH: 610 \\ Loss: 0.0157, W: 1.974, b: 1.048\n",
      "EPOCH: 620 \\ Loss: 0.0156, W: 1.976, b: 1.047\n",
      "EPOCH: 630 \\ Loss: 0.0156, W: 1.977, b: 1.047\n",
      "EPOCH: 640 \\ Loss: 0.0156, W: 1.978, b: 1.046\n",
      "EPOCH: 650 \\ Loss: 0.0156, W: 1.979, b: 1.045\n",
      "EPOCH: 660 \\ Loss: 0.0156, W: 1.980, b: 1.044\n",
      "EPOCH: 670 \\ Loss: 0.0156, W: 1.981, b: 1.044\n",
      "EPOCH: 680 \\ Loss: 0.0155, W: 1.982, b: 1.043\n",
      "EPOCH: 690 \\ Loss: 0.0155, W: 1.982, b: 1.042\n",
      "EPOCH: 700 \\ Loss: 0.0155, W: 1.983, b: 1.042\n",
      "EPOCH: 710 \\ Loss: 0.0155, W: 1.984, b: 1.041\n",
      "EPOCH: 720 \\ Loss: 0.0155, W: 1.984, b: 1.041\n",
      "EPOCH: 730 \\ Loss: 0.0155, W: 1.985, b: 1.041\n",
      "EPOCH: 740 \\ Loss: 0.0155, W: 1.985, b: 1.040\n",
      "EPOCH: 750 \\ Loss: 0.0155, W: 1.986, b: 1.040\n",
      "EPOCH: 760 \\ Loss: 0.0155, W: 1.986, b: 1.040\n",
      "EPOCH: 770 \\ Loss: 0.0155, W: 1.987, b: 1.039\n",
      "EPOCH: 780 \\ Loss: 0.0155, W: 1.987, b: 1.039\n",
      "EPOCH: 790 \\ Loss: 0.0155, W: 1.987, b: 1.039\n",
      "EPOCH: 800 \\ Loss: 0.0155, W: 1.988, b: 1.039\n",
      "EPOCH: 810 \\ Loss: 0.0155, W: 1.988, b: 1.038\n",
      "EPOCH: 820 \\ Loss: 0.0155, W: 1.988, b: 1.038\n",
      "EPOCH: 830 \\ Loss: 0.0155, W: 1.989, b: 1.038\n",
      "EPOCH: 840 \\ Loss: 0.0155, W: 1.989, b: 1.038\n",
      "EPOCH: 850 \\ Loss: 0.0155, W: 1.989, b: 1.038\n",
      "EPOCH: 860 \\ Loss: 0.0155, W: 1.989, b: 1.037\n",
      "EPOCH: 870 \\ Loss: 0.0155, W: 1.989, b: 1.037\n",
      "EPOCH: 880 \\ Loss: 0.0155, W: 1.990, b: 1.037\n",
      "EPOCH: 890 \\ Loss: 0.0155, W: 1.990, b: 1.037\n",
      "EPOCH: 900 \\ Loss: 0.0155, W: 1.990, b: 1.037\n",
      "EPOCH: 910 \\ Loss: 0.0155, W: 1.990, b: 1.037\n",
      "EPOCH: 920 \\ Loss: 0.0155, W: 1.990, b: 1.037\n",
      "EPOCH: 930 \\ Loss: 0.0155, W: 1.990, b: 1.037\n",
      "EPOCH: 940 \\ Loss: 0.0155, W: 1.990, b: 1.037\n",
      "EPOCH: 950 \\ Loss: 0.0155, W: 1.990, b: 1.037\n",
      "EPOCH: 960 \\ Loss: 0.0155, W: 1.991, b: 1.037\n",
      "EPOCH: 970 \\ Loss: 0.0155, W: 1.991, b: 1.036\n",
      "EPOCH: 980 \\ Loss: 0.0155, W: 1.991, b: 1.036\n",
      "EPOCH: 990 \\ Loss: 0.0155, W: 1.991, b: 1.036\n",
      "FINAL PARAMETERS: W=1.991, b=1.036\n",
      "True params: W=2, b=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:49: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:49: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\Stein\\AppData\\Local\\Temp\\ipykernel_4720\\218697554.py:49: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print(f'EPOCH: {epoch:02d} \\ Loss: {loss.item():.4f}, W: {W.item():.3f}, b: {b.item():.3f}')\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "D_in = 1\n",
    "D_out = 1\n",
    "X = torch.randn(N,D_in)\n",
    "\n",
    "true_W = torch.tensor([[2.0]])\n",
    "true_b = torch.tensor(1.0)\n",
    "y = X @ true_W + true_b + torch.randn(N, D_out) * 0.1\n",
    "\n",
    "W = torch.randn(D_in, D_out, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "print(f\"Init. W: {W}\")\n",
    "print(f\"Init. b: {b}\")\n",
    "\n",
    "def model(X):\n",
    "    return X @ W + b\n",
    "\n",
    "y_hat = model(X)\n",
    "\n",
    "print(f\"Prediction y hat: {y_hat[:3]}\")\n",
    "print(f\"True y: {y[:3]}\")\n",
    "\n",
    "error = y_hat - y\n",
    "squared_error = error ** 2\n",
    "loss = squared_error.mean()\n",
    "\n",
    "print(f\"LOSS: {loss}\")\n",
    "\n",
    "loss.backward()\n",
    "print(W.grad)\n",
    "print(b.grad)\n",
    "\n",
    "learning_rate, epochs = 0.01, 1000\n",
    "W, b = torch.randn(1,1,requires_grad=True), torch.rand(1, requires_grad=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_hat = X @ W + b\n",
    "    loss = torch.mean((y_hat - y) ** 2)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        W -= learning_rate * W.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    W.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'EPOCH: {epoch:02d} \\ Loss: {loss.item():.4f}, W: {W.item():.3f}, b: {b.item():.3f}')\n",
    "\n",
    "print(f'FINAL PARAMETERS: W={W.item():.3f}, b={b.item():.3f}')\n",
    "print(f'True params: W=2, b=1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "871b2182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers Weight: Parameter containing:\n",
      "tensor([[-0.2661]], requires_grad=True)\n",
      "Layers bias: Parameter containing:\n",
      "tensor([-0.8872], requires_grad=True)\n",
      "OUTPUT of linear: tensor([[-1.1818],\n",
      "        [-0.7010],\n",
      "        [-1.2062]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "D_in = 1\n",
    "D_out = 1\n",
    "\n",
    "linear_layer = torch.nn.Linear(in_features=D_in, out_features=D_out)\n",
    "\n",
    "print(f'Layers Weight: {linear_layer.weight}')\n",
    "print(f'Layers bias: {linear_layer.bias}')\n",
    "\n",
    "y_hatnn = linear_layer(X)\n",
    "\n",
    "print(f\"OUTPUT of linear: {y_hatnn[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "1ac29eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegressionModel(\n",
      "  (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Epoch 00 / 1000 | Loss: 1.7669\n",
      "Epoch 01 / 1000 | Loss: 1.7251\n",
      "Epoch 02 / 1000 | Loss: 1.6838\n",
      "Epoch 03 / 1000 | Loss: 1.6431\n",
      "Epoch 04 / 1000 | Loss: 1.6029\n",
      "Epoch 05 / 1000 | Loss: 1.5633\n",
      "Epoch 06 / 1000 | Loss: 1.5243\n",
      "Epoch 07 / 1000 | Loss: 1.4859\n",
      "Epoch 08 / 1000 | Loss: 1.4480\n",
      "Epoch 09 / 1000 | Loss: 1.4108\n",
      "Epoch 10 / 1000 | Loss: 1.3741\n",
      "Epoch 11 / 1000 | Loss: 1.3380\n",
      "Epoch 12 / 1000 | Loss: 1.3026\n",
      "Epoch 13 / 1000 | Loss: 1.2677\n",
      "Epoch 14 / 1000 | Loss: 1.2335\n",
      "Epoch 15 / 1000 | Loss: 1.1999\n",
      "Epoch 16 / 1000 | Loss: 1.1669\n",
      "Epoch 17 / 1000 | Loss: 1.1345\n",
      "Epoch 18 / 1000 | Loss: 1.1028\n",
      "Epoch 19 / 1000 | Loss: 1.0717\n",
      "Epoch 20 / 1000 | Loss: 1.0412\n",
      "Epoch 21 / 1000 | Loss: 1.0113\n",
      "Epoch 22 / 1000 | Loss: 0.9821\n",
      "Epoch 23 / 1000 | Loss: 0.9535\n",
      "Epoch 24 / 1000 | Loss: 0.9255\n",
      "Epoch 25 / 1000 | Loss: 0.8981\n",
      "Epoch 26 / 1000 | Loss: 0.8714\n",
      "Epoch 27 / 1000 | Loss: 0.8452\n",
      "Epoch 28 / 1000 | Loss: 0.8197\n",
      "Epoch 29 / 1000 | Loss: 0.7947\n",
      "Epoch 30 / 1000 | Loss: 0.7704\n",
      "Epoch 31 / 1000 | Loss: 0.7467\n",
      "Epoch 32 / 1000 | Loss: 0.7235\n",
      "Epoch 33 / 1000 | Loss: 0.7010\n",
      "Epoch 34 / 1000 | Loss: 0.6790\n",
      "Epoch 35 / 1000 | Loss: 0.6576\n",
      "Epoch 36 / 1000 | Loss: 0.6367\n",
      "Epoch 37 / 1000 | Loss: 0.6164\n",
      "Epoch 38 / 1000 | Loss: 0.5967\n",
      "Epoch 39 / 1000 | Loss: 0.5775\n",
      "Epoch 40 / 1000 | Loss: 0.5588\n",
      "Epoch 41 / 1000 | Loss: 0.5406\n",
      "Epoch 42 / 1000 | Loss: 0.5230\n",
      "Epoch 43 / 1000 | Loss: 0.5059\n",
      "Epoch 44 / 1000 | Loss: 0.4893\n",
      "Epoch 45 / 1000 | Loss: 0.4731\n",
      "Epoch 46 / 1000 | Loss: 0.4575\n",
      "Epoch 47 / 1000 | Loss: 0.4423\n",
      "Epoch 48 / 1000 | Loss: 0.4276\n",
      "Epoch 49 / 1000 | Loss: 0.4134\n",
      "Epoch 50 / 1000 | Loss: 0.3995\n",
      "Epoch 51 / 1000 | Loss: 0.3862\n",
      "Epoch 52 / 1000 | Loss: 0.3732\n",
      "Epoch 53 / 1000 | Loss: 0.3607\n",
      "Epoch 54 / 1000 | Loss: 0.3486\n",
      "Epoch 55 / 1000 | Loss: 0.3368\n",
      "Epoch 56 / 1000 | Loss: 0.3255\n",
      "Epoch 57 / 1000 | Loss: 0.3146\n",
      "Epoch 58 / 1000 | Loss: 0.3040\n",
      "Epoch 59 / 1000 | Loss: 0.2937\n",
      "Epoch 60 / 1000 | Loss: 0.2839\n",
      "Epoch 61 / 1000 | Loss: 0.2744\n",
      "Epoch 62 / 1000 | Loss: 0.2652\n",
      "Epoch 63 / 1000 | Loss: 0.2563\n",
      "Epoch 64 / 1000 | Loss: 0.2477\n",
      "Epoch 65 / 1000 | Loss: 0.2395\n",
      "Epoch 66 / 1000 | Loss: 0.2316\n",
      "Epoch 67 / 1000 | Loss: 0.2239\n",
      "Epoch 68 / 1000 | Loss: 0.2165\n",
      "Epoch 69 / 1000 | Loss: 0.2094\n",
      "Epoch 70 / 1000 | Loss: 0.2026\n",
      "Epoch 71 / 1000 | Loss: 0.1960\n",
      "Epoch 72 / 1000 | Loss: 0.1897\n",
      "Epoch 73 / 1000 | Loss: 0.1836\n",
      "Epoch 74 / 1000 | Loss: 0.1777\n",
      "Epoch 75 / 1000 | Loss: 0.1721\n",
      "Epoch 76 / 1000 | Loss: 0.1667\n",
      "Epoch 77 / 1000 | Loss: 0.1615\n",
      "Epoch 78 / 1000 | Loss: 0.1565\n",
      "Epoch 79 / 1000 | Loss: 0.1517\n",
      "Epoch 80 / 1000 | Loss: 0.1470\n",
      "Epoch 81 / 1000 | Loss: 0.1426\n",
      "Epoch 82 / 1000 | Loss: 0.1383\n",
      "Epoch 83 / 1000 | Loss: 0.1343\n",
      "Epoch 84 / 1000 | Loss: 0.1303\n",
      "Epoch 85 / 1000 | Loss: 0.1266\n",
      "Epoch 86 / 1000 | Loss: 0.1230\n",
      "Epoch 87 / 1000 | Loss: 0.1195\n",
      "Epoch 88 / 1000 | Loss: 0.1162\n",
      "Epoch 89 / 1000 | Loss: 0.1130\n",
      "Epoch 90 / 1000 | Loss: 0.1099\n",
      "Epoch 91 / 1000 | Loss: 0.1069\n",
      "Epoch 92 / 1000 | Loss: 0.1041\n",
      "Epoch 93 / 1000 | Loss: 0.1014\n",
      "Epoch 94 / 1000 | Loss: 0.0988\n",
      "Epoch 95 / 1000 | Loss: 0.0963\n",
      "Epoch 96 / 1000 | Loss: 0.0939\n",
      "Epoch 97 / 1000 | Loss: 0.0916\n",
      "Epoch 98 / 1000 | Loss: 0.0894\n",
      "Epoch 99 / 1000 | Loss: 0.0873\n",
      "Epoch 100 / 1000 | Loss: 0.0853\n",
      "Epoch 101 / 1000 | Loss: 0.0833\n",
      "Epoch 102 / 1000 | Loss: 0.0814\n",
      "Epoch 103 / 1000 | Loss: 0.0796\n",
      "Epoch 104 / 1000 | Loss: 0.0779\n",
      "Epoch 105 / 1000 | Loss: 0.0762\n",
      "Epoch 106 / 1000 | Loss: 0.0746\n",
      "Epoch 107 / 1000 | Loss: 0.0731\n",
      "Epoch 108 / 1000 | Loss: 0.0716\n",
      "Epoch 109 / 1000 | Loss: 0.0702\n",
      "Epoch 110 / 1000 | Loss: 0.0688\n",
      "Epoch 111 / 1000 | Loss: 0.0675\n",
      "Epoch 112 / 1000 | Loss: 0.0662\n",
      "Epoch 113 / 1000 | Loss: 0.0650\n",
      "Epoch 114 / 1000 | Loss: 0.0638\n",
      "Epoch 115 / 1000 | Loss: 0.0627\n",
      "Epoch 116 / 1000 | Loss: 0.0616\n",
      "Epoch 117 / 1000 | Loss: 0.0605\n",
      "Epoch 118 / 1000 | Loss: 0.0595\n",
      "Epoch 119 / 1000 | Loss: 0.0585\n",
      "Epoch 120 / 1000 | Loss: 0.0575\n",
      "Epoch 121 / 1000 | Loss: 0.0566\n",
      "Epoch 122 / 1000 | Loss: 0.0557\n",
      "Epoch 123 / 1000 | Loss: 0.0549\n",
      "Epoch 124 / 1000 | Loss: 0.0540\n",
      "Epoch 125 / 1000 | Loss: 0.0532\n",
      "Epoch 126 / 1000 | Loss: 0.0524\n",
      "Epoch 127 / 1000 | Loss: 0.0516\n",
      "Epoch 128 / 1000 | Loss: 0.0509\n",
      "Epoch 129 / 1000 | Loss: 0.0502\n",
      "Epoch 130 / 1000 | Loss: 0.0494\n",
      "Epoch 131 / 1000 | Loss: 0.0488\n",
      "Epoch 132 / 1000 | Loss: 0.0481\n",
      "Epoch 133 / 1000 | Loss: 0.0474\n",
      "Epoch 134 / 1000 | Loss: 0.0468\n",
      "Epoch 135 / 1000 | Loss: 0.0462\n",
      "Epoch 136 / 1000 | Loss: 0.0456\n",
      "Epoch 137 / 1000 | Loss: 0.0450\n",
      "Epoch 138 / 1000 | Loss: 0.0444\n",
      "Epoch 139 / 1000 | Loss: 0.0439\n",
      "Epoch 140 / 1000 | Loss: 0.0433\n",
      "Epoch 141 / 1000 | Loss: 0.0428\n",
      "Epoch 142 / 1000 | Loss: 0.0423\n",
      "Epoch 143 / 1000 | Loss: 0.0418\n",
      "Epoch 144 / 1000 | Loss: 0.0413\n",
      "Epoch 145 / 1000 | Loss: 0.0408\n",
      "Epoch 146 / 1000 | Loss: 0.0403\n",
      "Epoch 147 / 1000 | Loss: 0.0398\n",
      "Epoch 148 / 1000 | Loss: 0.0394\n",
      "Epoch 149 / 1000 | Loss: 0.0389\n",
      "Epoch 150 / 1000 | Loss: 0.0385\n",
      "Epoch 151 / 1000 | Loss: 0.0380\n",
      "Epoch 152 / 1000 | Loss: 0.0376\n",
      "Epoch 153 / 1000 | Loss: 0.0372\n",
      "Epoch 154 / 1000 | Loss: 0.0368\n",
      "Epoch 155 / 1000 | Loss: 0.0364\n",
      "Epoch 156 / 1000 | Loss: 0.0360\n",
      "Epoch 157 / 1000 | Loss: 0.0356\n",
      "Epoch 158 / 1000 | Loss: 0.0352\n",
      "Epoch 159 / 1000 | Loss: 0.0348\n",
      "Epoch 160 / 1000 | Loss: 0.0345\n",
      "Epoch 161 / 1000 | Loss: 0.0341\n",
      "Epoch 162 / 1000 | Loss: 0.0338\n",
      "Epoch 163 / 1000 | Loss: 0.0334\n",
      "Epoch 164 / 1000 | Loss: 0.0331\n",
      "Epoch 165 / 1000 | Loss: 0.0327\n",
      "Epoch 166 / 1000 | Loss: 0.0324\n",
      "Epoch 167 / 1000 | Loss: 0.0321\n",
      "Epoch 168 / 1000 | Loss: 0.0318\n",
      "Epoch 169 / 1000 | Loss: 0.0314\n",
      "Epoch 170 / 1000 | Loss: 0.0311\n",
      "Epoch 171 / 1000 | Loss: 0.0308\n",
      "Epoch 172 / 1000 | Loss: 0.0305\n",
      "Epoch 173 / 1000 | Loss: 0.0302\n",
      "Epoch 174 / 1000 | Loss: 0.0300\n",
      "Epoch 175 / 1000 | Loss: 0.0297\n",
      "Epoch 176 / 1000 | Loss: 0.0294\n",
      "Epoch 177 / 1000 | Loss: 0.0291\n",
      "Epoch 178 / 1000 | Loss: 0.0288\n",
      "Epoch 179 / 1000 | Loss: 0.0286\n",
      "Epoch 180 / 1000 | Loss: 0.0283\n",
      "Epoch 181 / 1000 | Loss: 0.0281\n",
      "Epoch 182 / 1000 | Loss: 0.0278\n",
      "Epoch 183 / 1000 | Loss: 0.0276\n",
      "Epoch 184 / 1000 | Loss: 0.0273\n",
      "Epoch 185 / 1000 | Loss: 0.0271\n",
      "Epoch 186 / 1000 | Loss: 0.0269\n",
      "Epoch 187 / 1000 | Loss: 0.0266\n",
      "Epoch 188 / 1000 | Loss: 0.0264\n",
      "Epoch 189 / 1000 | Loss: 0.0262\n",
      "Epoch 190 / 1000 | Loss: 0.0260\n",
      "Epoch 191 / 1000 | Loss: 0.0257\n",
      "Epoch 192 / 1000 | Loss: 0.0255\n",
      "Epoch 193 / 1000 | Loss: 0.0253\n",
      "Epoch 194 / 1000 | Loss: 0.0251\n",
      "Epoch 195 / 1000 | Loss: 0.0249\n",
      "Epoch 196 / 1000 | Loss: 0.0247\n",
      "Epoch 197 / 1000 | Loss: 0.0245\n",
      "Epoch 198 / 1000 | Loss: 0.0243\n",
      "Epoch 199 / 1000 | Loss: 0.0242\n",
      "Epoch 200 / 1000 | Loss: 0.0240\n",
      "Epoch 201 / 1000 | Loss: 0.0238\n",
      "Epoch 202 / 1000 | Loss: 0.0236\n",
      "Epoch 203 / 1000 | Loss: 0.0234\n",
      "Epoch 204 / 1000 | Loss: 0.0233\n",
      "Epoch 205 / 1000 | Loss: 0.0231\n",
      "Epoch 206 / 1000 | Loss: 0.0229\n",
      "Epoch 207 / 1000 | Loss: 0.0228\n",
      "Epoch 208 / 1000 | Loss: 0.0226\n",
      "Epoch 209 / 1000 | Loss: 0.0225\n",
      "Epoch 210 / 1000 | Loss: 0.0223\n",
      "Epoch 211 / 1000 | Loss: 0.0222\n",
      "Epoch 212 / 1000 | Loss: 0.0220\n",
      "Epoch 213 / 1000 | Loss: 0.0219\n",
      "Epoch 214 / 1000 | Loss: 0.0217\n",
      "Epoch 215 / 1000 | Loss: 0.0216\n",
      "Epoch 216 / 1000 | Loss: 0.0215\n",
      "Epoch 217 / 1000 | Loss: 0.0213\n",
      "Epoch 218 / 1000 | Loss: 0.0212\n",
      "Epoch 219 / 1000 | Loss: 0.0211\n",
      "Epoch 220 / 1000 | Loss: 0.0210\n",
      "Epoch 221 / 1000 | Loss: 0.0208\n",
      "Epoch 222 / 1000 | Loss: 0.0207\n",
      "Epoch 223 / 1000 | Loss: 0.0206\n",
      "Epoch 224 / 1000 | Loss: 0.0205\n",
      "Epoch 225 / 1000 | Loss: 0.0204\n",
      "Epoch 226 / 1000 | Loss: 0.0203\n",
      "Epoch 227 / 1000 | Loss: 0.0201\n",
      "Epoch 228 / 1000 | Loss: 0.0200\n",
      "Epoch 229 / 1000 | Loss: 0.0199\n",
      "Epoch 230 / 1000 | Loss: 0.0198\n",
      "Epoch 231 / 1000 | Loss: 0.0197\n",
      "Epoch 232 / 1000 | Loss: 0.0196\n",
      "Epoch 233 / 1000 | Loss: 0.0195\n",
      "Epoch 234 / 1000 | Loss: 0.0194\n",
      "Epoch 235 / 1000 | Loss: 0.0194\n",
      "Epoch 236 / 1000 | Loss: 0.0193\n",
      "Epoch 237 / 1000 | Loss: 0.0192\n",
      "Epoch 238 / 1000 | Loss: 0.0191\n",
      "Epoch 239 / 1000 | Loss: 0.0190\n",
      "Epoch 240 / 1000 | Loss: 0.0189\n",
      "Epoch 241 / 1000 | Loss: 0.0188\n",
      "Epoch 242 / 1000 | Loss: 0.0188\n",
      "Epoch 243 / 1000 | Loss: 0.0187\n",
      "Epoch 244 / 1000 | Loss: 0.0186\n",
      "Epoch 245 / 1000 | Loss: 0.0185\n",
      "Epoch 246 / 1000 | Loss: 0.0185\n",
      "Epoch 247 / 1000 | Loss: 0.0184\n",
      "Epoch 248 / 1000 | Loss: 0.0183\n",
      "Epoch 249 / 1000 | Loss: 0.0183\n",
      "Epoch 250 / 1000 | Loss: 0.0182\n",
      "Epoch 251 / 1000 | Loss: 0.0181\n",
      "Epoch 252 / 1000 | Loss: 0.0181\n",
      "Epoch 253 / 1000 | Loss: 0.0180\n",
      "Epoch 254 / 1000 | Loss: 0.0179\n",
      "Epoch 255 / 1000 | Loss: 0.0179\n",
      "Epoch 256 / 1000 | Loss: 0.0178\n",
      "Epoch 257 / 1000 | Loss: 0.0178\n",
      "Epoch 258 / 1000 | Loss: 0.0177\n",
      "Epoch 259 / 1000 | Loss: 0.0176\n",
      "Epoch 260 / 1000 | Loss: 0.0176\n",
      "Epoch 261 / 1000 | Loss: 0.0175\n",
      "Epoch 262 / 1000 | Loss: 0.0175\n",
      "Epoch 263 / 1000 | Loss: 0.0174\n",
      "Epoch 264 / 1000 | Loss: 0.0174\n",
      "Epoch 265 / 1000 | Loss: 0.0173\n",
      "Epoch 266 / 1000 | Loss: 0.0173\n",
      "Epoch 267 / 1000 | Loss: 0.0172\n",
      "Epoch 268 / 1000 | Loss: 0.0172\n",
      "Epoch 269 / 1000 | Loss: 0.0172\n",
      "Epoch 270 / 1000 | Loss: 0.0171\n",
      "Epoch 271 / 1000 | Loss: 0.0171\n",
      "Epoch 272 / 1000 | Loss: 0.0170\n",
      "Epoch 273 / 1000 | Loss: 0.0170\n",
      "Epoch 274 / 1000 | Loss: 0.0170\n",
      "Epoch 275 / 1000 | Loss: 0.0169\n",
      "Epoch 276 / 1000 | Loss: 0.0169\n",
      "Epoch 277 / 1000 | Loss: 0.0168\n",
      "Epoch 278 / 1000 | Loss: 0.0168\n",
      "Epoch 279 / 1000 | Loss: 0.0168\n",
      "Epoch 280 / 1000 | Loss: 0.0167\n",
      "Epoch 281 / 1000 | Loss: 0.0167\n",
      "Epoch 282 / 1000 | Loss: 0.0167\n",
      "Epoch 283 / 1000 | Loss: 0.0166\n",
      "Epoch 284 / 1000 | Loss: 0.0166\n",
      "Epoch 285 / 1000 | Loss: 0.0166\n",
      "Epoch 286 / 1000 | Loss: 0.0166\n",
      "Epoch 287 / 1000 | Loss: 0.0165\n",
      "Epoch 288 / 1000 | Loss: 0.0165\n",
      "Epoch 289 / 1000 | Loss: 0.0165\n",
      "Epoch 290 / 1000 | Loss: 0.0164\n",
      "Epoch 291 / 1000 | Loss: 0.0164\n",
      "Epoch 292 / 1000 | Loss: 0.0164\n",
      "Epoch 293 / 1000 | Loss: 0.0164\n",
      "Epoch 294 / 1000 | Loss: 0.0163\n",
      "Epoch 295 / 1000 | Loss: 0.0163\n",
      "Epoch 296 / 1000 | Loss: 0.0163\n",
      "Epoch 297 / 1000 | Loss: 0.0163\n",
      "Epoch 298 / 1000 | Loss: 0.0163\n",
      "Epoch 299 / 1000 | Loss: 0.0162\n",
      "Epoch 300 / 1000 | Loss: 0.0162\n",
      "Epoch 301 / 1000 | Loss: 0.0162\n",
      "Epoch 302 / 1000 | Loss: 0.0162\n",
      "Epoch 303 / 1000 | Loss: 0.0162\n",
      "Epoch 304 / 1000 | Loss: 0.0161\n",
      "Epoch 305 / 1000 | Loss: 0.0161\n",
      "Epoch 306 / 1000 | Loss: 0.0161\n",
      "Epoch 307 / 1000 | Loss: 0.0161\n",
      "Epoch 308 / 1000 | Loss: 0.0161\n",
      "Epoch 309 / 1000 | Loss: 0.0161\n",
      "Epoch 310 / 1000 | Loss: 0.0160\n",
      "Epoch 311 / 1000 | Loss: 0.0160\n",
      "Epoch 312 / 1000 | Loss: 0.0160\n",
      "Epoch 313 / 1000 | Loss: 0.0160\n",
      "Epoch 314 / 1000 | Loss: 0.0160\n",
      "Epoch 315 / 1000 | Loss: 0.0160\n",
      "Epoch 316 / 1000 | Loss: 0.0160\n",
      "Epoch 317 / 1000 | Loss: 0.0159\n",
      "Epoch 318 / 1000 | Loss: 0.0159\n",
      "Epoch 319 / 1000 | Loss: 0.0159\n",
      "Epoch 320 / 1000 | Loss: 0.0159\n",
      "Epoch 321 / 1000 | Loss: 0.0159\n",
      "Epoch 322 / 1000 | Loss: 0.0159\n",
      "Epoch 323 / 1000 | Loss: 0.0159\n",
      "Epoch 324 / 1000 | Loss: 0.0159\n",
      "Epoch 325 / 1000 | Loss: 0.0158\n",
      "Epoch 326 / 1000 | Loss: 0.0158\n",
      "Epoch 327 / 1000 | Loss: 0.0158\n",
      "Epoch 328 / 1000 | Loss: 0.0158\n",
      "Epoch 329 / 1000 | Loss: 0.0158\n",
      "Epoch 330 / 1000 | Loss: 0.0158\n",
      "Epoch 331 / 1000 | Loss: 0.0158\n",
      "Epoch 332 / 1000 | Loss: 0.0158\n",
      "Epoch 333 / 1000 | Loss: 0.0158\n",
      "Epoch 334 / 1000 | Loss: 0.0158\n",
      "Epoch 335 / 1000 | Loss: 0.0158\n",
      "Epoch 336 / 1000 | Loss: 0.0157\n",
      "Epoch 337 / 1000 | Loss: 0.0157\n",
      "Epoch 338 / 1000 | Loss: 0.0157\n",
      "Epoch 339 / 1000 | Loss: 0.0157\n",
      "Epoch 340 / 1000 | Loss: 0.0157\n",
      "Epoch 341 / 1000 | Loss: 0.0157\n",
      "Epoch 342 / 1000 | Loss: 0.0157\n",
      "Epoch 343 / 1000 | Loss: 0.0157\n",
      "Epoch 344 / 1000 | Loss: 0.0157\n",
      "Epoch 345 / 1000 | Loss: 0.0157\n",
      "Epoch 346 / 1000 | Loss: 0.0157\n",
      "Epoch 347 / 1000 | Loss: 0.0157\n",
      "Epoch 348 / 1000 | Loss: 0.0157\n",
      "Epoch 349 / 1000 | Loss: 0.0157\n",
      "Epoch 350 / 1000 | Loss: 0.0157\n",
      "Epoch 351 / 1000 | Loss: 0.0156\n",
      "Epoch 352 / 1000 | Loss: 0.0156\n",
      "Epoch 353 / 1000 | Loss: 0.0156\n",
      "Epoch 354 / 1000 | Loss: 0.0156\n",
      "Epoch 355 / 1000 | Loss: 0.0156\n",
      "Epoch 356 / 1000 | Loss: 0.0156\n",
      "Epoch 357 / 1000 | Loss: 0.0156\n",
      "Epoch 358 / 1000 | Loss: 0.0156\n",
      "Epoch 359 / 1000 | Loss: 0.0156\n",
      "Epoch 360 / 1000 | Loss: 0.0156\n",
      "Epoch 361 / 1000 | Loss: 0.0156\n",
      "Epoch 362 / 1000 | Loss: 0.0156\n",
      "Epoch 363 / 1000 | Loss: 0.0156\n",
      "Epoch 364 / 1000 | Loss: 0.0156\n",
      "Epoch 365 / 1000 | Loss: 0.0156\n",
      "Epoch 366 / 1000 | Loss: 0.0156\n",
      "Epoch 367 / 1000 | Loss: 0.0156\n",
      "Epoch 368 / 1000 | Loss: 0.0156\n",
      "Epoch 369 / 1000 | Loss: 0.0156\n",
      "Epoch 370 / 1000 | Loss: 0.0156\n",
      "Epoch 371 / 1000 | Loss: 0.0156\n",
      "Epoch 372 / 1000 | Loss: 0.0156\n",
      "Epoch 373 / 1000 | Loss: 0.0156\n",
      "Epoch 374 / 1000 | Loss: 0.0156\n",
      "Epoch 375 / 1000 | Loss: 0.0156\n",
      "Epoch 376 / 1000 | Loss: 0.0156\n",
      "Epoch 377 / 1000 | Loss: 0.0156\n",
      "Epoch 378 / 1000 | Loss: 0.0156\n",
      "Epoch 379 / 1000 | Loss: 0.0156\n",
      "Epoch 380 / 1000 | Loss: 0.0155\n",
      "Epoch 381 / 1000 | Loss: 0.0155\n",
      "Epoch 382 / 1000 | Loss: 0.0155\n",
      "Epoch 383 / 1000 | Loss: 0.0155\n",
      "Epoch 384 / 1000 | Loss: 0.0155\n",
      "Epoch 385 / 1000 | Loss: 0.0155\n",
      "Epoch 386 / 1000 | Loss: 0.0155\n",
      "Epoch 387 / 1000 | Loss: 0.0155\n",
      "Epoch 388 / 1000 | Loss: 0.0155\n",
      "Epoch 389 / 1000 | Loss: 0.0155\n",
      "Epoch 390 / 1000 | Loss: 0.0155\n",
      "Epoch 391 / 1000 | Loss: 0.0155\n",
      "Epoch 392 / 1000 | Loss: 0.0155\n",
      "Epoch 393 / 1000 | Loss: 0.0155\n",
      "Epoch 394 / 1000 | Loss: 0.0155\n",
      "Epoch 395 / 1000 | Loss: 0.0155\n",
      "Epoch 396 / 1000 | Loss: 0.0155\n",
      "Epoch 397 / 1000 | Loss: 0.0155\n",
      "Epoch 398 / 1000 | Loss: 0.0155\n",
      "Epoch 399 / 1000 | Loss: 0.0155\n",
      "Epoch 400 / 1000 | Loss: 0.0155\n",
      "Epoch 401 / 1000 | Loss: 0.0155\n",
      "Epoch 402 / 1000 | Loss: 0.0155\n",
      "Epoch 403 / 1000 | Loss: 0.0155\n",
      "Epoch 404 / 1000 | Loss: 0.0155\n",
      "Epoch 405 / 1000 | Loss: 0.0155\n",
      "Epoch 406 / 1000 | Loss: 0.0155\n",
      "Epoch 407 / 1000 | Loss: 0.0155\n",
      "Epoch 408 / 1000 | Loss: 0.0155\n",
      "Epoch 409 / 1000 | Loss: 0.0155\n",
      "Epoch 410 / 1000 | Loss: 0.0155\n",
      "Epoch 411 / 1000 | Loss: 0.0155\n",
      "Epoch 412 / 1000 | Loss: 0.0155\n",
      "Epoch 413 / 1000 | Loss: 0.0155\n",
      "Epoch 414 / 1000 | Loss: 0.0155\n",
      "Epoch 415 / 1000 | Loss: 0.0155\n",
      "Epoch 416 / 1000 | Loss: 0.0155\n",
      "Epoch 417 / 1000 | Loss: 0.0155\n",
      "Epoch 418 / 1000 | Loss: 0.0155\n",
      "Epoch 419 / 1000 | Loss: 0.0155\n",
      "Epoch 420 / 1000 | Loss: 0.0155\n",
      "Epoch 421 / 1000 | Loss: 0.0155\n",
      "Epoch 422 / 1000 | Loss: 0.0155\n",
      "Epoch 423 / 1000 | Loss: 0.0155\n",
      "Epoch 424 / 1000 | Loss: 0.0155\n",
      "Epoch 425 / 1000 | Loss: 0.0155\n",
      "Epoch 426 / 1000 | Loss: 0.0155\n",
      "Epoch 427 / 1000 | Loss: 0.0155\n",
      "Epoch 428 / 1000 | Loss: 0.0155\n",
      "Epoch 429 / 1000 | Loss: 0.0155\n",
      "Epoch 430 / 1000 | Loss: 0.0155\n",
      "Epoch 431 / 1000 | Loss: 0.0155\n",
      "Epoch 432 / 1000 | Loss: 0.0155\n",
      "Epoch 433 / 1000 | Loss: 0.0155\n",
      "Epoch 434 / 1000 | Loss: 0.0155\n",
      "Epoch 435 / 1000 | Loss: 0.0155\n",
      "Epoch 436 / 1000 | Loss: 0.0155\n",
      "Epoch 437 / 1000 | Loss: 0.0155\n",
      "Epoch 438 / 1000 | Loss: 0.0155\n",
      "Epoch 439 / 1000 | Loss: 0.0155\n",
      "Epoch 440 / 1000 | Loss: 0.0155\n",
      "Epoch 441 / 1000 | Loss: 0.0155\n",
      "Epoch 442 / 1000 | Loss: 0.0155\n",
      "Epoch 443 / 1000 | Loss: 0.0155\n",
      "Epoch 444 / 1000 | Loss: 0.0155\n",
      "Epoch 445 / 1000 | Loss: 0.0155\n",
      "Epoch 446 / 1000 | Loss: 0.0155\n",
      "Epoch 447 / 1000 | Loss: 0.0155\n",
      "Epoch 448 / 1000 | Loss: 0.0155\n",
      "Epoch 449 / 1000 | Loss: 0.0155\n",
      "Epoch 450 / 1000 | Loss: 0.0155\n",
      "Epoch 451 / 1000 | Loss: 0.0155\n",
      "Epoch 452 / 1000 | Loss: 0.0155\n",
      "Epoch 453 / 1000 | Loss: 0.0155\n",
      "Epoch 454 / 1000 | Loss: 0.0155\n",
      "Epoch 455 / 1000 | Loss: 0.0155\n",
      "Epoch 456 / 1000 | Loss: 0.0155\n",
      "Epoch 457 / 1000 | Loss: 0.0155\n",
      "Epoch 458 / 1000 | Loss: 0.0155\n",
      "Epoch 459 / 1000 | Loss: 0.0155\n",
      "Epoch 460 / 1000 | Loss: 0.0155\n",
      "Epoch 461 / 1000 | Loss: 0.0155\n",
      "Epoch 462 / 1000 | Loss: 0.0155\n",
      "Epoch 463 / 1000 | Loss: 0.0155\n",
      "Epoch 464 / 1000 | Loss: 0.0155\n",
      "Epoch 465 / 1000 | Loss: 0.0155\n",
      "Epoch 466 / 1000 | Loss: 0.0155\n",
      "Epoch 467 / 1000 | Loss: 0.0155\n",
      "Epoch 468 / 1000 | Loss: 0.0155\n",
      "Epoch 469 / 1000 | Loss: 0.0155\n",
      "Epoch 470 / 1000 | Loss: 0.0155\n",
      "Epoch 471 / 1000 | Loss: 0.0155\n",
      "Epoch 472 / 1000 | Loss: 0.0155\n",
      "Epoch 473 / 1000 | Loss: 0.0155\n",
      "Epoch 474 / 1000 | Loss: 0.0155\n",
      "Epoch 475 / 1000 | Loss: 0.0155\n",
      "Epoch 476 / 1000 | Loss: 0.0155\n",
      "Epoch 477 / 1000 | Loss: 0.0155\n",
      "Epoch 478 / 1000 | Loss: 0.0155\n",
      "Epoch 479 / 1000 | Loss: 0.0155\n",
      "Epoch 480 / 1000 | Loss: 0.0155\n",
      "Epoch 481 / 1000 | Loss: 0.0155\n",
      "Epoch 482 / 1000 | Loss: 0.0155\n",
      "Epoch 483 / 1000 | Loss: 0.0155\n",
      "Epoch 484 / 1000 | Loss: 0.0155\n",
      "Epoch 485 / 1000 | Loss: 0.0155\n",
      "Epoch 486 / 1000 | Loss: 0.0155\n",
      "Epoch 487 / 1000 | Loss: 0.0155\n",
      "Epoch 488 / 1000 | Loss: 0.0155\n",
      "Epoch 489 / 1000 | Loss: 0.0155\n",
      "Epoch 490 / 1000 | Loss: 0.0155\n",
      "Epoch 491 / 1000 | Loss: 0.0155\n",
      "Epoch 492 / 1000 | Loss: 0.0155\n",
      "Epoch 493 / 1000 | Loss: 0.0155\n",
      "Epoch 494 / 1000 | Loss: 0.0155\n",
      "Epoch 495 / 1000 | Loss: 0.0155\n",
      "Epoch 496 / 1000 | Loss: 0.0155\n",
      "Epoch 497 / 1000 | Loss: 0.0155\n",
      "Epoch 498 / 1000 | Loss: 0.0155\n",
      "Epoch 499 / 1000 | Loss: 0.0155\n",
      "Epoch 500 / 1000 | Loss: 0.0155\n",
      "Epoch 501 / 1000 | Loss: 0.0155\n",
      "Epoch 502 / 1000 | Loss: 0.0155\n",
      "Epoch 503 / 1000 | Loss: 0.0155\n",
      "Epoch 504 / 1000 | Loss: 0.0155\n",
      "Epoch 505 / 1000 | Loss: 0.0155\n",
      "Epoch 506 / 1000 | Loss: 0.0155\n",
      "Epoch 507 / 1000 | Loss: 0.0155\n",
      "Epoch 508 / 1000 | Loss: 0.0155\n",
      "Epoch 509 / 1000 | Loss: 0.0155\n",
      "Epoch 510 / 1000 | Loss: 0.0155\n",
      "Epoch 511 / 1000 | Loss: 0.0155\n",
      "Epoch 512 / 1000 | Loss: 0.0155\n",
      "Epoch 513 / 1000 | Loss: 0.0155\n",
      "Epoch 514 / 1000 | Loss: 0.0155\n",
      "Epoch 515 / 1000 | Loss: 0.0155\n",
      "Epoch 516 / 1000 | Loss: 0.0155\n",
      "Epoch 517 / 1000 | Loss: 0.0155\n",
      "Epoch 518 / 1000 | Loss: 0.0155\n",
      "Epoch 519 / 1000 | Loss: 0.0155\n",
      "Epoch 520 / 1000 | Loss: 0.0155\n",
      "Epoch 521 / 1000 | Loss: 0.0155\n",
      "Epoch 522 / 1000 | Loss: 0.0155\n",
      "Epoch 523 / 1000 | Loss: 0.0155\n",
      "Epoch 524 / 1000 | Loss: 0.0155\n",
      "Epoch 525 / 1000 | Loss: 0.0155\n",
      "Epoch 526 / 1000 | Loss: 0.0155\n",
      "Epoch 527 / 1000 | Loss: 0.0155\n",
      "Epoch 528 / 1000 | Loss: 0.0155\n",
      "Epoch 529 / 1000 | Loss: 0.0155\n",
      "Epoch 530 / 1000 | Loss: 0.0155\n",
      "Epoch 531 / 1000 | Loss: 0.0155\n",
      "Epoch 532 / 1000 | Loss: 0.0155\n",
      "Epoch 533 / 1000 | Loss: 0.0155\n",
      "Epoch 534 / 1000 | Loss: 0.0155\n",
      "Epoch 535 / 1000 | Loss: 0.0155\n",
      "Epoch 536 / 1000 | Loss: 0.0155\n",
      "Epoch 537 / 1000 | Loss: 0.0155\n",
      "Epoch 538 / 1000 | Loss: 0.0155\n",
      "Epoch 539 / 1000 | Loss: 0.0155\n",
      "Epoch 540 / 1000 | Loss: 0.0155\n",
      "Epoch 541 / 1000 | Loss: 0.0155\n",
      "Epoch 542 / 1000 | Loss: 0.0155\n",
      "Epoch 543 / 1000 | Loss: 0.0155\n",
      "Epoch 544 / 1000 | Loss: 0.0155\n",
      "Epoch 545 / 1000 | Loss: 0.0155\n",
      "Epoch 546 / 1000 | Loss: 0.0155\n",
      "Epoch 547 / 1000 | Loss: 0.0155\n",
      "Epoch 548 / 1000 | Loss: 0.0155\n",
      "Epoch 549 / 1000 | Loss: 0.0155\n",
      "Epoch 550 / 1000 | Loss: 0.0155\n",
      "Epoch 551 / 1000 | Loss: 0.0155\n",
      "Epoch 552 / 1000 | Loss: 0.0155\n",
      "Epoch 553 / 1000 | Loss: 0.0155\n",
      "Epoch 554 / 1000 | Loss: 0.0155\n",
      "Epoch 555 / 1000 | Loss: 0.0155\n",
      "Epoch 556 / 1000 | Loss: 0.0155\n",
      "Epoch 557 / 1000 | Loss: 0.0155\n",
      "Epoch 558 / 1000 | Loss: 0.0155\n",
      "Epoch 559 / 1000 | Loss: 0.0155\n",
      "Epoch 560 / 1000 | Loss: 0.0155\n",
      "Epoch 561 / 1000 | Loss: 0.0155\n",
      "Epoch 562 / 1000 | Loss: 0.0155\n",
      "Epoch 563 / 1000 | Loss: 0.0155\n",
      "Epoch 564 / 1000 | Loss: 0.0155\n",
      "Epoch 565 / 1000 | Loss: 0.0155\n",
      "Epoch 566 / 1000 | Loss: 0.0155\n",
      "Epoch 567 / 1000 | Loss: 0.0155\n",
      "Epoch 568 / 1000 | Loss: 0.0155\n",
      "Epoch 569 / 1000 | Loss: 0.0155\n",
      "Epoch 570 / 1000 | Loss: 0.0155\n",
      "Epoch 571 / 1000 | Loss: 0.0155\n",
      "Epoch 572 / 1000 | Loss: 0.0155\n",
      "Epoch 573 / 1000 | Loss: 0.0155\n",
      "Epoch 574 / 1000 | Loss: 0.0155\n",
      "Epoch 575 / 1000 | Loss: 0.0155\n",
      "Epoch 576 / 1000 | Loss: 0.0155\n",
      "Epoch 577 / 1000 | Loss: 0.0155\n",
      "Epoch 578 / 1000 | Loss: 0.0155\n",
      "Epoch 579 / 1000 | Loss: 0.0155\n",
      "Epoch 580 / 1000 | Loss: 0.0155\n",
      "Epoch 581 / 1000 | Loss: 0.0155\n",
      "Epoch 582 / 1000 | Loss: 0.0155\n",
      "Epoch 583 / 1000 | Loss: 0.0155\n",
      "Epoch 584 / 1000 | Loss: 0.0155\n",
      "Epoch 585 / 1000 | Loss: 0.0155\n",
      "Epoch 586 / 1000 | Loss: 0.0155\n",
      "Epoch 587 / 1000 | Loss: 0.0155\n",
      "Epoch 588 / 1000 | Loss: 0.0155\n",
      "Epoch 589 / 1000 | Loss: 0.0155\n",
      "Epoch 590 / 1000 | Loss: 0.0155\n",
      "Epoch 591 / 1000 | Loss: 0.0155\n",
      "Epoch 592 / 1000 | Loss: 0.0155\n",
      "Epoch 593 / 1000 | Loss: 0.0155\n",
      "Epoch 594 / 1000 | Loss: 0.0155\n",
      "Epoch 595 / 1000 | Loss: 0.0155\n",
      "Epoch 596 / 1000 | Loss: 0.0155\n",
      "Epoch 597 / 1000 | Loss: 0.0155\n",
      "Epoch 598 / 1000 | Loss: 0.0155\n",
      "Epoch 599 / 1000 | Loss: 0.0155\n",
      "Epoch 600 / 1000 | Loss: 0.0155\n",
      "Epoch 601 / 1000 | Loss: 0.0155\n",
      "Epoch 602 / 1000 | Loss: 0.0155\n",
      "Epoch 603 / 1000 | Loss: 0.0155\n",
      "Epoch 604 / 1000 | Loss: 0.0155\n",
      "Epoch 605 / 1000 | Loss: 0.0155\n",
      "Epoch 606 / 1000 | Loss: 0.0155\n",
      "Epoch 607 / 1000 | Loss: 0.0155\n",
      "Epoch 608 / 1000 | Loss: 0.0155\n",
      "Epoch 609 / 1000 | Loss: 0.0155\n",
      "Epoch 610 / 1000 | Loss: 0.0155\n",
      "Epoch 611 / 1000 | Loss: 0.0155\n",
      "Epoch 612 / 1000 | Loss: 0.0155\n",
      "Epoch 613 / 1000 | Loss: 0.0155\n",
      "Epoch 614 / 1000 | Loss: 0.0155\n",
      "Epoch 615 / 1000 | Loss: 0.0155\n",
      "Epoch 616 / 1000 | Loss: 0.0155\n",
      "Epoch 617 / 1000 | Loss: 0.0155\n",
      "Epoch 618 / 1000 | Loss: 0.0155\n",
      "Epoch 619 / 1000 | Loss: 0.0155\n",
      "Epoch 620 / 1000 | Loss: 0.0155\n",
      "Epoch 621 / 1000 | Loss: 0.0155\n",
      "Epoch 622 / 1000 | Loss: 0.0155\n",
      "Epoch 623 / 1000 | Loss: 0.0155\n",
      "Epoch 624 / 1000 | Loss: 0.0155\n",
      "Epoch 625 / 1000 | Loss: 0.0155\n",
      "Epoch 626 / 1000 | Loss: 0.0155\n",
      "Epoch 627 / 1000 | Loss: 0.0155\n",
      "Epoch 628 / 1000 | Loss: 0.0155\n",
      "Epoch 629 / 1000 | Loss: 0.0155\n",
      "Epoch 630 / 1000 | Loss: 0.0155\n",
      "Epoch 631 / 1000 | Loss: 0.0155\n",
      "Epoch 632 / 1000 | Loss: 0.0155\n",
      "Epoch 633 / 1000 | Loss: 0.0155\n",
      "Epoch 634 / 1000 | Loss: 0.0155\n",
      "Epoch 635 / 1000 | Loss: 0.0155\n",
      "Epoch 636 / 1000 | Loss: 0.0155\n",
      "Epoch 637 / 1000 | Loss: 0.0155\n",
      "Epoch 638 / 1000 | Loss: 0.0155\n",
      "Epoch 639 / 1000 | Loss: 0.0155\n",
      "Epoch 640 / 1000 | Loss: 0.0155\n",
      "Epoch 641 / 1000 | Loss: 0.0155\n",
      "Epoch 642 / 1000 | Loss: 0.0155\n",
      "Epoch 643 / 1000 | Loss: 0.0155\n",
      "Epoch 644 / 1000 | Loss: 0.0155\n",
      "Epoch 645 / 1000 | Loss: 0.0155\n",
      "Epoch 646 / 1000 | Loss: 0.0155\n",
      "Epoch 647 / 1000 | Loss: 0.0155\n",
      "Epoch 648 / 1000 | Loss: 0.0155\n",
      "Epoch 649 / 1000 | Loss: 0.0155\n",
      "Epoch 650 / 1000 | Loss: 0.0155\n",
      "Epoch 651 / 1000 | Loss: 0.0155\n",
      "Epoch 652 / 1000 | Loss: 0.0155\n",
      "Epoch 653 / 1000 | Loss: 0.0155\n",
      "Epoch 654 / 1000 | Loss: 0.0155\n",
      "Epoch 655 / 1000 | Loss: 0.0155\n",
      "Epoch 656 / 1000 | Loss: 0.0155\n",
      "Epoch 657 / 1000 | Loss: 0.0155\n",
      "Epoch 658 / 1000 | Loss: 0.0155\n",
      "Epoch 659 / 1000 | Loss: 0.0155\n",
      "Epoch 660 / 1000 | Loss: 0.0155\n",
      "Epoch 661 / 1000 | Loss: 0.0155\n",
      "Epoch 662 / 1000 | Loss: 0.0155\n",
      "Epoch 663 / 1000 | Loss: 0.0155\n",
      "Epoch 664 / 1000 | Loss: 0.0155\n",
      "Epoch 665 / 1000 | Loss: 0.0155\n",
      "Epoch 666 / 1000 | Loss: 0.0155\n",
      "Epoch 667 / 1000 | Loss: 0.0155\n",
      "Epoch 668 / 1000 | Loss: 0.0155\n",
      "Epoch 669 / 1000 | Loss: 0.0155\n",
      "Epoch 670 / 1000 | Loss: 0.0155\n",
      "Epoch 671 / 1000 | Loss: 0.0155\n",
      "Epoch 672 / 1000 | Loss: 0.0155\n",
      "Epoch 673 / 1000 | Loss: 0.0155\n",
      "Epoch 674 / 1000 | Loss: 0.0155\n",
      "Epoch 675 / 1000 | Loss: 0.0155\n",
      "Epoch 676 / 1000 | Loss: 0.0155\n",
      "Epoch 677 / 1000 | Loss: 0.0155\n",
      "Epoch 678 / 1000 | Loss: 0.0155\n",
      "Epoch 679 / 1000 | Loss: 0.0155\n",
      "Epoch 680 / 1000 | Loss: 0.0155\n",
      "Epoch 681 / 1000 | Loss: 0.0155\n",
      "Epoch 682 / 1000 | Loss: 0.0155\n",
      "Epoch 683 / 1000 | Loss: 0.0155\n",
      "Epoch 684 / 1000 | Loss: 0.0155\n",
      "Epoch 685 / 1000 | Loss: 0.0155\n",
      "Epoch 686 / 1000 | Loss: 0.0155\n",
      "Epoch 687 / 1000 | Loss: 0.0155\n",
      "Epoch 688 / 1000 | Loss: 0.0155\n",
      "Epoch 689 / 1000 | Loss: 0.0155\n",
      "Epoch 690 / 1000 | Loss: 0.0155\n",
      "Epoch 691 / 1000 | Loss: 0.0155\n",
      "Epoch 692 / 1000 | Loss: 0.0155\n",
      "Epoch 693 / 1000 | Loss: 0.0155\n",
      "Epoch 694 / 1000 | Loss: 0.0155\n",
      "Epoch 695 / 1000 | Loss: 0.0155\n",
      "Epoch 696 / 1000 | Loss: 0.0155\n",
      "Epoch 697 / 1000 | Loss: 0.0155\n",
      "Epoch 698 / 1000 | Loss: 0.0155\n",
      "Epoch 699 / 1000 | Loss: 0.0155\n",
      "Epoch 700 / 1000 | Loss: 0.0155\n",
      "Epoch 701 / 1000 | Loss: 0.0155\n",
      "Epoch 702 / 1000 | Loss: 0.0155\n",
      "Epoch 703 / 1000 | Loss: 0.0155\n",
      "Epoch 704 / 1000 | Loss: 0.0155\n",
      "Epoch 705 / 1000 | Loss: 0.0155\n",
      "Epoch 706 / 1000 | Loss: 0.0155\n",
      "Epoch 707 / 1000 | Loss: 0.0155\n",
      "Epoch 708 / 1000 | Loss: 0.0155\n",
      "Epoch 709 / 1000 | Loss: 0.0155\n",
      "Epoch 710 / 1000 | Loss: 0.0155\n",
      "Epoch 711 / 1000 | Loss: 0.0155\n",
      "Epoch 712 / 1000 | Loss: 0.0155\n",
      "Epoch 713 / 1000 | Loss: 0.0155\n",
      "Epoch 714 / 1000 | Loss: 0.0155\n",
      "Epoch 715 / 1000 | Loss: 0.0155\n",
      "Epoch 716 / 1000 | Loss: 0.0155\n",
      "Epoch 717 / 1000 | Loss: 0.0155\n",
      "Epoch 718 / 1000 | Loss: 0.0155\n",
      "Epoch 719 / 1000 | Loss: 0.0155\n",
      "Epoch 720 / 1000 | Loss: 0.0155\n",
      "Epoch 721 / 1000 | Loss: 0.0155\n",
      "Epoch 722 / 1000 | Loss: 0.0155\n",
      "Epoch 723 / 1000 | Loss: 0.0155\n",
      "Epoch 724 / 1000 | Loss: 0.0155\n",
      "Epoch 725 / 1000 | Loss: 0.0155\n",
      "Epoch 726 / 1000 | Loss: 0.0155\n",
      "Epoch 727 / 1000 | Loss: 0.0155\n",
      "Epoch 728 / 1000 | Loss: 0.0155\n",
      "Epoch 729 / 1000 | Loss: 0.0155\n",
      "Epoch 730 / 1000 | Loss: 0.0155\n",
      "Epoch 731 / 1000 | Loss: 0.0155\n",
      "Epoch 732 / 1000 | Loss: 0.0155\n",
      "Epoch 733 / 1000 | Loss: 0.0155\n",
      "Epoch 734 / 1000 | Loss: 0.0155\n",
      "Epoch 735 / 1000 | Loss: 0.0155\n",
      "Epoch 736 / 1000 | Loss: 0.0155\n",
      "Epoch 737 / 1000 | Loss: 0.0155\n",
      "Epoch 738 / 1000 | Loss: 0.0155\n",
      "Epoch 739 / 1000 | Loss: 0.0155\n",
      "Epoch 740 / 1000 | Loss: 0.0155\n",
      "Epoch 741 / 1000 | Loss: 0.0155\n",
      "Epoch 742 / 1000 | Loss: 0.0155\n",
      "Epoch 743 / 1000 | Loss: 0.0155\n",
      "Epoch 744 / 1000 | Loss: 0.0155\n",
      "Epoch 745 / 1000 | Loss: 0.0155\n",
      "Epoch 746 / 1000 | Loss: 0.0155\n",
      "Epoch 747 / 1000 | Loss: 0.0155\n",
      "Epoch 748 / 1000 | Loss: 0.0155\n",
      "Epoch 749 / 1000 | Loss: 0.0155\n",
      "Epoch 750 / 1000 | Loss: 0.0155\n",
      "Epoch 751 / 1000 | Loss: 0.0155\n",
      "Epoch 752 / 1000 | Loss: 0.0155\n",
      "Epoch 753 / 1000 | Loss: 0.0155\n",
      "Epoch 754 / 1000 | Loss: 0.0155\n",
      "Epoch 755 / 1000 | Loss: 0.0155\n",
      "Epoch 756 / 1000 | Loss: 0.0155\n",
      "Epoch 757 / 1000 | Loss: 0.0155\n",
      "Epoch 758 / 1000 | Loss: 0.0155\n",
      "Epoch 759 / 1000 | Loss: 0.0155\n",
      "Epoch 760 / 1000 | Loss: 0.0155\n",
      "Epoch 761 / 1000 | Loss: 0.0155\n",
      "Epoch 762 / 1000 | Loss: 0.0155\n",
      "Epoch 763 / 1000 | Loss: 0.0155\n",
      "Epoch 764 / 1000 | Loss: 0.0155\n",
      "Epoch 765 / 1000 | Loss: 0.0155\n",
      "Epoch 766 / 1000 | Loss: 0.0155\n",
      "Epoch 767 / 1000 | Loss: 0.0155\n",
      "Epoch 768 / 1000 | Loss: 0.0155\n",
      "Epoch 769 / 1000 | Loss: 0.0155\n",
      "Epoch 770 / 1000 | Loss: 0.0155\n",
      "Epoch 771 / 1000 | Loss: 0.0155\n",
      "Epoch 772 / 1000 | Loss: 0.0155\n",
      "Epoch 773 / 1000 | Loss: 0.0155\n",
      "Epoch 774 / 1000 | Loss: 0.0155\n",
      "Epoch 775 / 1000 | Loss: 0.0155\n",
      "Epoch 776 / 1000 | Loss: 0.0155\n",
      "Epoch 777 / 1000 | Loss: 0.0155\n",
      "Epoch 778 / 1000 | Loss: 0.0155\n",
      "Epoch 779 / 1000 | Loss: 0.0155\n",
      "Epoch 780 / 1000 | Loss: 0.0155\n",
      "Epoch 781 / 1000 | Loss: 0.0155\n",
      "Epoch 782 / 1000 | Loss: 0.0155\n",
      "Epoch 783 / 1000 | Loss: 0.0155\n",
      "Epoch 784 / 1000 | Loss: 0.0155\n",
      "Epoch 785 / 1000 | Loss: 0.0155\n",
      "Epoch 786 / 1000 | Loss: 0.0155\n",
      "Epoch 787 / 1000 | Loss: 0.0155\n",
      "Epoch 788 / 1000 | Loss: 0.0155\n",
      "Epoch 789 / 1000 | Loss: 0.0155\n",
      "Epoch 790 / 1000 | Loss: 0.0155\n",
      "Epoch 791 / 1000 | Loss: 0.0155\n",
      "Epoch 792 / 1000 | Loss: 0.0155\n",
      "Epoch 793 / 1000 | Loss: 0.0155\n",
      "Epoch 794 / 1000 | Loss: 0.0155\n",
      "Epoch 795 / 1000 | Loss: 0.0155\n",
      "Epoch 796 / 1000 | Loss: 0.0155\n",
      "Epoch 797 / 1000 | Loss: 0.0155\n",
      "Epoch 798 / 1000 | Loss: 0.0155\n",
      "Epoch 799 / 1000 | Loss: 0.0155\n",
      "Epoch 800 / 1000 | Loss: 0.0155\n",
      "Epoch 801 / 1000 | Loss: 0.0155\n",
      "Epoch 802 / 1000 | Loss: 0.0155\n",
      "Epoch 803 / 1000 | Loss: 0.0155\n",
      "Epoch 804 / 1000 | Loss: 0.0155\n",
      "Epoch 805 / 1000 | Loss: 0.0155\n",
      "Epoch 806 / 1000 | Loss: 0.0155\n",
      "Epoch 807 / 1000 | Loss: 0.0155\n",
      "Epoch 808 / 1000 | Loss: 0.0155\n",
      "Epoch 809 / 1000 | Loss: 0.0155\n",
      "Epoch 810 / 1000 | Loss: 0.0155\n",
      "Epoch 811 / 1000 | Loss: 0.0155\n",
      "Epoch 812 / 1000 | Loss: 0.0155\n",
      "Epoch 813 / 1000 | Loss: 0.0155\n",
      "Epoch 814 / 1000 | Loss: 0.0155\n",
      "Epoch 815 / 1000 | Loss: 0.0155\n",
      "Epoch 816 / 1000 | Loss: 0.0155\n",
      "Epoch 817 / 1000 | Loss: 0.0155\n",
      "Epoch 818 / 1000 | Loss: 0.0155\n",
      "Epoch 819 / 1000 | Loss: 0.0155\n",
      "Epoch 820 / 1000 | Loss: 0.0155\n",
      "Epoch 821 / 1000 | Loss: 0.0155\n",
      "Epoch 822 / 1000 | Loss: 0.0155\n",
      "Epoch 823 / 1000 | Loss: 0.0155\n",
      "Epoch 824 / 1000 | Loss: 0.0155\n",
      "Epoch 825 / 1000 | Loss: 0.0155\n",
      "Epoch 826 / 1000 | Loss: 0.0155\n",
      "Epoch 827 / 1000 | Loss: 0.0155\n",
      "Epoch 828 / 1000 | Loss: 0.0155\n",
      "Epoch 829 / 1000 | Loss: 0.0155\n",
      "Epoch 830 / 1000 | Loss: 0.0155\n",
      "Epoch 831 / 1000 | Loss: 0.0155\n",
      "Epoch 832 / 1000 | Loss: 0.0155\n",
      "Epoch 833 / 1000 | Loss: 0.0155\n",
      "Epoch 834 / 1000 | Loss: 0.0155\n",
      "Epoch 835 / 1000 | Loss: 0.0155\n",
      "Epoch 836 / 1000 | Loss: 0.0155\n",
      "Epoch 837 / 1000 | Loss: 0.0155\n",
      "Epoch 838 / 1000 | Loss: 0.0155\n",
      "Epoch 839 / 1000 | Loss: 0.0155\n",
      "Epoch 840 / 1000 | Loss: 0.0155\n",
      "Epoch 841 / 1000 | Loss: 0.0155\n",
      "Epoch 842 / 1000 | Loss: 0.0155\n",
      "Epoch 843 / 1000 | Loss: 0.0155\n",
      "Epoch 844 / 1000 | Loss: 0.0155\n",
      "Epoch 845 / 1000 | Loss: 0.0155\n",
      "Epoch 846 / 1000 | Loss: 0.0155\n",
      "Epoch 847 / 1000 | Loss: 0.0155\n",
      "Epoch 848 / 1000 | Loss: 0.0155\n",
      "Epoch 849 / 1000 | Loss: 0.0155\n",
      "Epoch 850 / 1000 | Loss: 0.0155\n",
      "Epoch 851 / 1000 | Loss: 0.0155\n",
      "Epoch 852 / 1000 | Loss: 0.0155\n",
      "Epoch 853 / 1000 | Loss: 0.0155\n",
      "Epoch 854 / 1000 | Loss: 0.0155\n",
      "Epoch 855 / 1000 | Loss: 0.0155\n",
      "Epoch 856 / 1000 | Loss: 0.0155\n",
      "Epoch 857 / 1000 | Loss: 0.0155\n",
      "Epoch 858 / 1000 | Loss: 0.0155\n",
      "Epoch 859 / 1000 | Loss: 0.0155\n",
      "Epoch 860 / 1000 | Loss: 0.0155\n",
      "Epoch 861 / 1000 | Loss: 0.0155\n",
      "Epoch 862 / 1000 | Loss: 0.0155\n",
      "Epoch 863 / 1000 | Loss: 0.0155\n",
      "Epoch 864 / 1000 | Loss: 0.0155\n",
      "Epoch 865 / 1000 | Loss: 0.0155\n",
      "Epoch 866 / 1000 | Loss: 0.0155\n",
      "Epoch 867 / 1000 | Loss: 0.0155\n",
      "Epoch 868 / 1000 | Loss: 0.0155\n",
      "Epoch 869 / 1000 | Loss: 0.0155\n",
      "Epoch 870 / 1000 | Loss: 0.0155\n",
      "Epoch 871 / 1000 | Loss: 0.0155\n",
      "Epoch 872 / 1000 | Loss: 0.0155\n",
      "Epoch 873 / 1000 | Loss: 0.0155\n",
      "Epoch 874 / 1000 | Loss: 0.0155\n",
      "Epoch 875 / 1000 | Loss: 0.0155\n",
      "Epoch 876 / 1000 | Loss: 0.0155\n",
      "Epoch 877 / 1000 | Loss: 0.0155\n",
      "Epoch 878 / 1000 | Loss: 0.0155\n",
      "Epoch 879 / 1000 | Loss: 0.0155\n",
      "Epoch 880 / 1000 | Loss: 0.0155\n",
      "Epoch 881 / 1000 | Loss: 0.0155\n",
      "Epoch 882 / 1000 | Loss: 0.0155\n",
      "Epoch 883 / 1000 | Loss: 0.0155\n",
      "Epoch 884 / 1000 | Loss: 0.0155\n",
      "Epoch 885 / 1000 | Loss: 0.0155\n",
      "Epoch 886 / 1000 | Loss: 0.0155\n",
      "Epoch 887 / 1000 | Loss: 0.0155\n",
      "Epoch 888 / 1000 | Loss: 0.0155\n",
      "Epoch 889 / 1000 | Loss: 0.0155\n",
      "Epoch 890 / 1000 | Loss: 0.0155\n",
      "Epoch 891 / 1000 | Loss: 0.0155\n",
      "Epoch 892 / 1000 | Loss: 0.0155\n",
      "Epoch 893 / 1000 | Loss: 0.0155\n",
      "Epoch 894 / 1000 | Loss: 0.0155\n",
      "Epoch 895 / 1000 | Loss: 0.0155\n",
      "Epoch 896 / 1000 | Loss: 0.0155\n",
      "Epoch 897 / 1000 | Loss: 0.0155\n",
      "Epoch 898 / 1000 | Loss: 0.0155\n",
      "Epoch 899 / 1000 | Loss: 0.0155\n",
      "Epoch 900 / 1000 | Loss: 0.0155\n",
      "Epoch 901 / 1000 | Loss: 0.0155\n",
      "Epoch 902 / 1000 | Loss: 0.0155\n",
      "Epoch 903 / 1000 | Loss: 0.0155\n",
      "Epoch 904 / 1000 | Loss: 0.0155\n",
      "Epoch 905 / 1000 | Loss: 0.0155\n",
      "Epoch 906 / 1000 | Loss: 0.0155\n",
      "Epoch 907 / 1000 | Loss: 0.0155\n",
      "Epoch 908 / 1000 | Loss: 0.0155\n",
      "Epoch 909 / 1000 | Loss: 0.0155\n",
      "Epoch 910 / 1000 | Loss: 0.0155\n",
      "Epoch 911 / 1000 | Loss: 0.0155\n",
      "Epoch 912 / 1000 | Loss: 0.0155\n",
      "Epoch 913 / 1000 | Loss: 0.0155\n",
      "Epoch 914 / 1000 | Loss: 0.0155\n",
      "Epoch 915 / 1000 | Loss: 0.0155\n",
      "Epoch 916 / 1000 | Loss: 0.0155\n",
      "Epoch 917 / 1000 | Loss: 0.0155\n",
      "Epoch 918 / 1000 | Loss: 0.0155\n",
      "Epoch 919 / 1000 | Loss: 0.0155\n",
      "Epoch 920 / 1000 | Loss: 0.0155\n",
      "Epoch 921 / 1000 | Loss: 0.0155\n",
      "Epoch 922 / 1000 | Loss: 0.0155\n",
      "Epoch 923 / 1000 | Loss: 0.0155\n",
      "Epoch 924 / 1000 | Loss: 0.0155\n",
      "Epoch 925 / 1000 | Loss: 0.0155\n",
      "Epoch 926 / 1000 | Loss: 0.0155\n",
      "Epoch 927 / 1000 | Loss: 0.0155\n",
      "Epoch 928 / 1000 | Loss: 0.0155\n",
      "Epoch 929 / 1000 | Loss: 0.0155\n",
      "Epoch 930 / 1000 | Loss: 0.0155\n",
      "Epoch 931 / 1000 | Loss: 0.0155\n",
      "Epoch 932 / 1000 | Loss: 0.0155\n",
      "Epoch 933 / 1000 | Loss: 0.0155\n",
      "Epoch 934 / 1000 | Loss: 0.0155\n",
      "Epoch 935 / 1000 | Loss: 0.0155\n",
      "Epoch 936 / 1000 | Loss: 0.0155\n",
      "Epoch 937 / 1000 | Loss: 0.0155\n",
      "Epoch 938 / 1000 | Loss: 0.0155\n",
      "Epoch 939 / 1000 | Loss: 0.0155\n",
      "Epoch 940 / 1000 | Loss: 0.0155\n",
      "Epoch 941 / 1000 | Loss: 0.0155\n",
      "Epoch 942 / 1000 | Loss: 0.0155\n",
      "Epoch 943 / 1000 | Loss: 0.0155\n",
      "Epoch 944 / 1000 | Loss: 0.0155\n",
      "Epoch 945 / 1000 | Loss: 0.0155\n",
      "Epoch 946 / 1000 | Loss: 0.0155\n",
      "Epoch 947 / 1000 | Loss: 0.0155\n",
      "Epoch 948 / 1000 | Loss: 0.0155\n",
      "Epoch 949 / 1000 | Loss: 0.0155\n",
      "Epoch 950 / 1000 | Loss: 0.0155\n",
      "Epoch 951 / 1000 | Loss: 0.0155\n",
      "Epoch 952 / 1000 | Loss: 0.0155\n",
      "Epoch 953 / 1000 | Loss: 0.0155\n",
      "Epoch 954 / 1000 | Loss: 0.0155\n",
      "Epoch 955 / 1000 | Loss: 0.0155\n",
      "Epoch 956 / 1000 | Loss: 0.0155\n",
      "Epoch 957 / 1000 | Loss: 0.0155\n",
      "Epoch 958 / 1000 | Loss: 0.0155\n",
      "Epoch 959 / 1000 | Loss: 0.0155\n",
      "Epoch 960 / 1000 | Loss: 0.0155\n",
      "Epoch 961 / 1000 | Loss: 0.0155\n",
      "Epoch 962 / 1000 | Loss: 0.0155\n",
      "Epoch 963 / 1000 | Loss: 0.0155\n",
      "Epoch 964 / 1000 | Loss: 0.0155\n",
      "Epoch 965 / 1000 | Loss: 0.0155\n",
      "Epoch 966 / 1000 | Loss: 0.0155\n",
      "Epoch 967 / 1000 | Loss: 0.0155\n",
      "Epoch 968 / 1000 | Loss: 0.0155\n",
      "Epoch 969 / 1000 | Loss: 0.0155\n",
      "Epoch 970 / 1000 | Loss: 0.0155\n",
      "Epoch 971 / 1000 | Loss: 0.0155\n",
      "Epoch 972 / 1000 | Loss: 0.0155\n",
      "Epoch 973 / 1000 | Loss: 0.0155\n",
      "Epoch 974 / 1000 | Loss: 0.0155\n",
      "Epoch 975 / 1000 | Loss: 0.0155\n",
      "Epoch 976 / 1000 | Loss: 0.0155\n",
      "Epoch 977 / 1000 | Loss: 0.0155\n",
      "Epoch 978 / 1000 | Loss: 0.0155\n",
      "Epoch 979 / 1000 | Loss: 0.0155\n",
      "Epoch 980 / 1000 | Loss: 0.0155\n",
      "Epoch 981 / 1000 | Loss: 0.0155\n",
      "Epoch 982 / 1000 | Loss: 0.0155\n",
      "Epoch 983 / 1000 | Loss: 0.0155\n",
      "Epoch 984 / 1000 | Loss: 0.0155\n",
      "Epoch 985 / 1000 | Loss: 0.0155\n",
      "Epoch 986 / 1000 | Loss: 0.0155\n",
      "Epoch 987 / 1000 | Loss: 0.0155\n",
      "Epoch 988 / 1000 | Loss: 0.0155\n",
      "Epoch 989 / 1000 | Loss: 0.0155\n",
      "Epoch 990 / 1000 | Loss: 0.0155\n",
      "Epoch 991 / 1000 | Loss: 0.0155\n",
      "Epoch 992 / 1000 | Loss: 0.0155\n",
      "Epoch 993 / 1000 | Loss: 0.0155\n",
      "Epoch 994 / 1000 | Loss: 0.0155\n",
      "Epoch 995 / 1000 | Loss: 0.0155\n",
      "Epoch 996 / 1000 | Loss: 0.0155\n",
      "Epoch 997 / 1000 | Loss: 0.0155\n",
      "Epoch 998 / 1000 | Loss: 0.0155\n",
      "Epoch 999 / 1000 | Loss: 0.0155\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear_layer = nn.Linear(in_features, out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "model = LinearRegressionModel(in_features=1, out_features=1)\n",
    "print(model)\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn=nn.MSELoss()\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    y_hat = model(X)\n",
    "    loss = loss_fn(y_hat, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch:02d} / {epochs} | Loss: {loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
